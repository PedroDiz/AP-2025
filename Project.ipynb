{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25WcywhEl74u"
   },
   "source": [
    "#Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TVB2infcQJZc",
    "outputId": "93f87021-c756-4fc7-8e67-6b610bafee40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: grad-cam in c:\\users\\pedro\\anaconda3\\lib\\site-packages (1.5.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\pedro\\anaconda3\\lib\\site-packages (from grad-cam) (1.26.4)\n",
      "Requirement already satisfied: Pillow in c:\\users\\pedro\\anaconda3\\lib\\site-packages (from grad-cam) (10.3.0)\n",
      "Requirement already satisfied: torch>=1.7.1 in c:\\users\\pedro\\anaconda3\\lib\\site-packages (from grad-cam) (2.7.0)\n",
      "Requirement already satisfied: torchvision>=0.8.2 in c:\\users\\pedro\\anaconda3\\lib\\site-packages (from grad-cam) (0.22.0)\n",
      "Requirement already satisfied: ttach in c:\\users\\pedro\\anaconda3\\lib\\site-packages (from grad-cam) (0.0.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pedro\\anaconda3\\lib\\site-packages (from grad-cam) (4.66.4)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\pedro\\anaconda3\\lib\\site-packages (from grad-cam) (4.11.0.86)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\pedro\\anaconda3\\lib\\site-packages (from grad-cam) (3.8.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\pedro\\anaconda3\\lib\\site-packages (from grad-cam) (1.4.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\pedro\\anaconda3\\lib\\site-packages (from torch>=1.7.1->grad-cam) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\pedro\\anaconda3\\lib\\site-packages (from torch>=1.7.1->grad-cam) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\pedro\\anaconda3\\lib\\site-packages (from torch>=1.7.1->grad-cam) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\pedro\\anaconda3\\lib\\site-packages (from torch>=1.7.1->grad-cam) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\pedro\\anaconda3\\lib\\site-packages (from torch>=1.7.1->grad-cam) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\pedro\\anaconda3\\lib\\site-packages (from torch>=1.7.1->grad-cam) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\pedro\\anaconda3\\lib\\site-packages (from torch>=1.7.1->grad-cam) (69.5.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\pedro\\anaconda3\\lib\\site-packages (from matplotlib->grad-cam) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\pedro\\anaconda3\\lib\\site-packages (from matplotlib->grad-cam) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\pedro\\anaconda3\\lib\\site-packages (from matplotlib->grad-cam) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\pedro\\anaconda3\\lib\\site-packages (from matplotlib->grad-cam) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pedro\\anaconda3\\lib\\site-packages (from matplotlib->grad-cam) (23.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\pedro\\anaconda3\\lib\\site-packages (from matplotlib->grad-cam) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\pedro\\anaconda3\\lib\\site-packages (from matplotlib->grad-cam) (2.9.0.post0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\pedro\\anaconda3\\lib\\site-packages (from scikit-learn->grad-cam) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\pedro\\anaconda3\\lib\\site-packages (from scikit-learn->grad-cam) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\pedro\\anaconda3\\lib\\site-packages (from scikit-learn->grad-cam) (2.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\pedro\\anaconda3\\lib\\site-packages (from tqdm->grad-cam) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pedro\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->grad-cam) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\pedro\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.7.1->grad-cam) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\pedro\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.7.1->grad-cam) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install grad-cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "vS8sDjV6Xcuv"
   },
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "# Data handling\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PyTorch & vision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import timm\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torch.nn as nn\n",
    "from torch.amp import autocast, GradScaler\n",
    "# Grad-CAM\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "\n",
    "\n",
    "# Scikit-learn metrics & utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, roc_curve,recall_score,\n",
    "    f1_score,\n",
    "    balanced_accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    classification_report,\n",
    "    precision_recall_curve,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "XyluRC2YKkBP"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def shades_of_gray(img: Image.Image, illuminant: str = \"gray_world\", p: float = 6.0) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Applies Shades-of-Gray color constancy to a PIL image.\n",
    "\n",
    "    Args:\n",
    "        img:      PIL.Image in RGB mode.\n",
    "        illuminant: Only \"gray_world\" is supported here.\n",
    "        p:        Minkowski norm power (commonly between 4 and 8).\n",
    "\n",
    "    Returns:\n",
    "        A new PIL.Image with color-corrected pixels.\n",
    "    \"\"\"\n",
    "    assert img.mode == \"RGB\", \"Image must be in RGB mode\"\n",
    "    # 1) Convert to float32 array, scale [0,1]\n",
    "    arr = np.asarray(img).astype(np.float32) / 255.0\n",
    "\n",
    "    # 2) Compute the Minkowski p‚Äênorm per channel\n",
    "    #    m_c = ( mean( channel^p ) )^(1/p)\n",
    "    m = np.mean(arr**p, axis=(0,1))**(1.0/p)  # shape (3,)\n",
    "\n",
    "    # 3) Compute scaling factors so that after correction the illuminant is gray\n",
    "    #    scale = mean(m) / m_c\n",
    "    mean_m = np.mean(m)\n",
    "    scale = mean_m / m\n",
    "\n",
    "    # 4) Apply scales\n",
    "    arr_corrected = arr * scale[None,None,:]\n",
    "\n",
    "    # 5) Clip back to [0,1], convert to uint8\n",
    "    arr_corrected = np.clip(arr_corrected, 0.0, 1.0)\n",
    "    img_out = Image.fromarray((arr_corrected * 255).astype(np.uint8), mode=\"RGB\")\n",
    "    return img_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"checkpoints\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_SCRIPT = \"./prepare_data.py\"\n",
    "PATH_ZIP = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Uncomment this code and substitute the variables to run in colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount = True)\n",
    "# Substitute this variable (PATH_ZIP) with the folder with the zip files containing the images, downloaded from github\n",
    "PATH_ZIP = \"/content/drive/MyDrive/ZIPS_DATA\"\n",
    "# Substitute this variable (PATH_SCRIPT) with the file prepare_data.py\n",
    "PATH_SCRIPT = \"/content/drive/MyDrive/prepare_data.py\" \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B0GuramtlF8E",
    "outputId": "5e0c4871-f490-4c47-a56e-c305bdda5d63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting images from ZIP files in: ./\n",
      "Extracted 11730 images to 'ISIC_IMAGES'.\n",
      "Fetching metadata from ISIC API...\n",
      "Collected 11720 metadata entries.\n",
      "Building dataframe from metadata and filtering by extracted images...\n",
      "Train: 6117 patients, 8180 samples\n",
      "  Benign 81.0% | Malignant 19.0%\n",
      "\n",
      "Val: 1311 patients, 1694 samples\n",
      "  Benign 82.8% | Malignant 17.2%\n",
      "\n",
      "Test: 1311 patients, 1697 samples\n",
      "  Benign 81.9% | Malignant 18.1%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python \"{PATH_SCRIPT}\" --zip_dir \"{PATH_ZIP}\" --output_dir data_v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3pAAg87FQDg"
   },
   "source": [
    "# Dataset & Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OeEz_3eYye7"
   },
   "source": [
    "The dataset is imbalanced, with the majority of images representing benign (non-cancerous) skin lesions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "CSNGhvtNYY_Z",
    "outputId": "7953742f-5409-476c-f850-84b222a3f397"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data_v1/lesions_train.csv\")  # or lesions_val.csv, lesions.csv, etc.\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "df['label'].value_counts().sort_index().plot(kind='bar', color=[\"skyblue\", \"salmon\"])\n",
    "\n",
    "plt.title(\"Class Distribution (Train Set)\")\n",
    "plt.xlabel(\"Class Label (0 = Benign, 1 = Malignant)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AhsbkyiC-JAY"
   },
   "source": [
    "This custom `SkinCancerDataset` class wraps our image DataFrame for PyTorch:\n",
    "\n",
    "- **`dataframe`**: Pandas DataFrame containing `file`, `patient`, and `label` columns.  \n",
    "- **`image_dir`**: Directory where the images (`<isic_id>.jpg`) are stored.  \n",
    "- **`transform`**: A `torchvision.transforms.Compose` object for preprocessing and (optionally) augmentations.  \n",
    "- **`return_filename`**: If `True`, `__getitem__` returns `(image, label, filename)`, useful for logging ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "LvPWGcm_jIaH"
   },
   "outputs": [],
   "source": [
    "class SkinCancerDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, transform=None, return_filename=False):\n",
    "        self.df = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.return_filename = return_filename\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.image_dir, row[\"file\"])\n",
    "        label = torch.tensor(row[\"label\"], dtype=torch.float32)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.return_filename:\n",
    "            return image, label, row[\"file\"]\n",
    "        else:\n",
    "            return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L9FobtGl9C46"
   },
   "source": [
    "Augmentation and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "QM0H8fNa7smy"
   },
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "   # transforms.Lambda(lambda img: shades_of_gray(img, illuminant=\"gray_world\", p=6.0)),\n",
    "    transforms.RandomResizedCrop(224),   # or 300 for B3\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(0.1, 0.1, 0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "   # transforms.Lambda(lambda img: shades_of_gray(img, illuminant=\"gray_world\", p=6.0)),\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IF273kgGqWii"
   },
   "source": [
    "Wrap `SkinCancerDataset` in a PyTorch `DataLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "hCM35q_KjQnn"
   },
   "outputs": [],
   "source": [
    "PATH = \"/content/ISIC_IMAGES\"\n",
    "\n",
    "# Load CSVs generated by prepare_data.py\n",
    "train_df = pd.read_csv(\"data_v1/lesions_train.csv\")\n",
    "val_df   = pd.read_csv(\"data_v1/lesions_val.csv\")\n",
    "test_df = pd.read_csv(\"data_v1/lesions_test.csv\")\n",
    "\n",
    "train_dataset = SkinCancerDataset(\n",
    "    train_df,\n",
    "    PATH,\n",
    "    transform=train_transform,\n",
    "    return_filename=False\n",
    ")\n",
    "\n",
    "# Compute weights for class-balanced sampling (60/40 ratio)\n",
    "p_m = train_df[\"label\"].mean()\n",
    "w_benign    = 0.6 / (1.0 - p_m)\n",
    "w_malignant = 0.4 / p_m\n",
    "\n",
    "sample_weights = train_df[\"label\"].map({\n",
    "    0: w_benign,\n",
    "    1: w_malignant\n",
    "}).values\n",
    "\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=len(sample_weights),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    sampler=sampler,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_dataset = SkinCancerDataset(\n",
    "    val_df,\n",
    "    PATH,\n",
    "    transform=val_transform,\n",
    "    return_filename=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "\n",
    "test_dataset = SkinCancerDataset(\n",
    "    test_df,\n",
    "    PATH,\n",
    "    transform=val_transform,\n",
    "    return_filename=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mVfnTjfJ7u-E"
   },
   "source": [
    "# Model, Loss, Optimizer & Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9G7fDq9_OIv"
   },
   "source": [
    "Focal Binary Cross‚ÄêEntropy (Focal BCE) loss down‚Äêweights easy examples and focuses training on hard, misclassified samples.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "IW5o73pveE3S"
   },
   "outputs": [],
   "source": [
    "class FocalBCELoss(nn.Module):\n",
    "    def __init__(self, alpha=0.75, gamma=0.5, reduction=\"mean\", use_focal=True):\n",
    "        \"\"\"\n",
    "        alpha: weight for the positive (malignant) class (0 < alpha < 1)\n",
    "               smaller alpha ‚Üí less emphasis on positives\n",
    "        gamma: focusing parameter (‚â•0), smaller ‚Üí less focus on hard examples\n",
    "        reduction: 'mean' or 'sum'\n",
    "        use_focal: if False, falls back to plain BCEWithLogitsLoss\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.use_focal = use_focal\n",
    "\n",
    "        if self.use_focal:\n",
    "            # keep one BCE-with-logits loss per example for focal computation\n",
    "            self.bce = nn.BCEWithLogitsLoss(reduction='none')\n",
    "        else:\n",
    "            # if not using focal, just use standard BCEWithLogitsLoss with reduction\n",
    "            self.bce = nn.BCEWithLogitsLoss(reduction=self.reduction)\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # logits: raw model outputs (no sigmoid), shape [batch_size]\n",
    "        # targets: ground-truth labels (0.0 or 1.0), shape [batch_size]\n",
    "\n",
    "        if not self.use_focal:\n",
    "            # simple binary cross-entropy on logits\n",
    "            return self.bce(logits, targets)\n",
    "\n",
    "        # 1. Compute per-example BCE loss (no reduction)\n",
    "        bce_loss = self.bce(logits, targets)  # shape [batch_size]\n",
    "\n",
    "        # 2. Convert logits to probabilities in [0,1]\n",
    "        prob = torch.sigmoid(logits)          # shape [batch_size]\n",
    "\n",
    "        # 3. p_t: model's probability of the true class\n",
    "        #    for positive: prob; for negative: 1 - prob\n",
    "        p_t = prob * targets + (1 - prob) * (1 - targets)\n",
    "\n",
    "        # 4. alpha factor: alpha for positives, (1 - alpha) for negatives\n",
    "        alpha_factor = self.alpha * targets + (1 - self.alpha) * (1 - targets)\n",
    "\n",
    "        # 5. focal factor: (1 - p_t)^gamma ‚Äî focuses on hard (uncertain) examples\n",
    "        focal_factor = (1 - p_t) ** self.gamma\n",
    "\n",
    "        # 6. Combine: alpha * focal * bce\n",
    "        loss = alpha_factor * focal_factor * bce_loss  # shape [batch_size]\n",
    "\n",
    "        # 7. Reduce to scalar\n",
    "        if self.reduction == \"mean\":\n",
    "            return loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss  # return per-example losses if reduction is None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_qN0uzl_lXQ"
   },
   "source": [
    "**Training configuration:**  \n",
    "- **Loss:** Focal Binary Cross‚ÄêEntropy.  \n",
    "- **Optimizer:** AdamW (learning rate = 3e-4, weight decay = 1e-4).  \n",
    "- **LR Scheduler:** Cosine annealing over 30 epochs (`T_max=30`).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156,
     "referenced_widgets": [
      "44aef92d50654c11ac0e2b1381356f72",
      "3d1d8de42f0849bf9788e289478e8bfa",
      "f638ba6378504fd9a9e6a2cb79086e23",
      "38d1f87310274329a63985e9ff882c4d",
      "576848c740094aac9b310c497da9bb03",
      "89830d71704449439a71a2e309dbca09",
      "e3f5c3ab67b94e409c72f51d704c8869",
      "6c7fad346ca740bfa807fc5be8399ce2",
      "e8cfee561f1d4ea9b3207603b7443535",
      "2003a0be287c4d16adb940d3b7a0ae3c",
      "8fd9c0423e814e6192edc836606a0746"
     ]
    },
    "id": "g8kyLdPs-RSX",
    "outputId": "50638ad7-4875-4597-c8e3-bfede2f0826c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pedro\\anaconda3\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ‚Äî Hyper-parameters ‚Äî\n",
    "n_epochs      = 20\n",
    "freeze_epochs = 3\n",
    "lr_head       = 3e-4\n",
    "lr_backbone   = 1e-4\n",
    "weight_decay  = 1e-4\n",
    "accum_steps   = 2  # 32 √ó 2 = effective batch 64\n",
    "\n",
    "# ‚Äî Model, loss, device ‚Äî\n",
    "model = timm.create_model(\n",
    "    \"efficientnet_b0\",# For b3 - efficientnet_b3\n",
    "    pretrained=True,\n",
    "    num_classes=1\n",
    ")\n",
    "criterion = FocalBCELoss(gamma=2, alpha=0.6)\n",
    "device    = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# ‚Äî 1) Freeze stem + block0 for warm-up ‚Äî\n",
    "for p in model.conv_stem.parameters():\n",
    "    p.requires_grad = False\n",
    "for p in model.blocks[0].parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# ‚Äî 2) Build optimizer with two LR groups ‚Äî\n",
    "head_params     = list(model.classifier.parameters())\n",
    "backbone_params = [p for n, p in model.named_parameters() if \"classifier\" not in n]\n",
    "\n",
    "optimizer = AdamW(\n",
    "    [\n",
    "        {\"params\": head_params,     \"lr\": lr_head},\n",
    "        {\"params\": backbone_params, \"lr\": lr_backbone}\n",
    "    ],\n",
    "    weight_decay=weight_decay\n",
    ")\n",
    "\n",
    "# ‚Äî 3) AMP scaler & scheduler ‚Äî\n",
    "scaler    = GradScaler()\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=n_epochs)\n",
    "\n",
    "# ‚Äî Reset GPU stats if on CUDA ‚Äî\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CPAWITFSNx3"
   },
   "source": [
    "# Training & Validation Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "N4aLrmlUvyxz"
   },
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "\n",
    "\n",
    "def apply_mixup_or_cutmix(x, y, alpha=0.2, mode=None):\n",
    "    \"\"\"\n",
    "    Applies MixUp or CutMix to a batch of images and labels.\n",
    "\n",
    "    Args:\n",
    "        x (Tensor): batch of images (B, C, H, W)\n",
    "        y (Tensor): batch of labels (B,)\n",
    "        alpha (float): beta distribution parameter\n",
    "        mode (str or None): 'mixup', 'cutmix', or None\n",
    "\n",
    "    Returns:\n",
    "        x_aug (Tensor): augmented images\n",
    "        y_a (Tensor): original labels\n",
    "        y_b (Tensor): shuffled labels\n",
    "        lam (float): mix ratio\n",
    "    \"\"\"\n",
    "    if mode is None:\n",
    "        return x, y, y, 1.0  # no mixup/cutmix\n",
    "\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    rand_index = torch.randperm(x.size(0)).to(x.device)\n",
    "    y_a = y\n",
    "    y_b = y[rand_index]\n",
    "    x2 = x[rand_index]\n",
    "\n",
    "    if mode == 'mixup':\n",
    "        x = lam * x + (1 - lam) * x2\n",
    "    elif mode == 'cutmix':\n",
    "        bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n",
    "        x[:, :, bbx1:bbx2, bby1:bby2] = x2[:, :, bbx1:bbx2, bby1:bby2]\n",
    "        lam = 1 - ((bbx2 - bbx1) * (bby1 - bby2) / (x.size(-1) * x.size(-2)))\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid mode: {mode}\")\n",
    "\n",
    "    return x, y_a, y_b, lam\n",
    "\n",
    "def apply_random_mix(x, y, alpha=0.4, prob_mixup=0.5, prob_cutmix=0.3):\n",
    "    \"\"\"\n",
    "    Randomly applies MixUp, CutMix, or nothing to the batch.\n",
    "    \"\"\"\n",
    "    p = np.random.rand()\n",
    "    if p < prob_mixup:\n",
    "        return apply_mixup_or_cutmix(x, y, alpha, mode=\"mixup\")\n",
    "    elif p < prob_mixup + prob_cutmix:\n",
    "        return apply_mixup_or_cutmix(x, y, alpha, mode=\"cutmix\")\n",
    "    else:\n",
    "        return x, y, y, 1.0  # no mixing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "DPyf0Em5jKrg"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(loader, model, criterion, optimizer, scaler, device, accum_steps, use_mixup=False):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    logits_all, probs_all, labels_all = [], [], []\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    pbar = tqdm(loader, desc=\"  [Train]   \", leave=False)\n",
    "    for i, (X, y) in enumerate(pbar):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        with autocast(device_type=\"cuda\"):\n",
    "            if use_mixup:\n",
    "                X_m, y_a, y_b, lam = apply_mixup_or_cutmix(X, y, mode=\"mixup\")\n",
    "                logits = model(X_m).squeeze()\n",
    "                raw_loss = lam * criterion(logits, y_a.float()) + (1 - lam) * criterion(logits, y_b.float())\n",
    "            else:\n",
    "                logits = model(X).squeeze()\n",
    "                raw_loss = criterion(logits, y.float())\n",
    "\n",
    "            loss = raw_loss / accum_steps\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (i + 1) % accum_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        batch_size = X.size(0)\n",
    "        running_loss += raw_loss.item() * batch_size\n",
    "        logits_all.extend(logits.detach().cpu().numpy())\n",
    "        probs_all.extend(torch.sigmoid(logits).detach().cpu().numpy())\n",
    "        labels_all.extend(y.cpu().numpy())\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            \"loss\": f\"{raw_loss.item():.3f}\",\n",
    "            \"lr\": optimizer.param_groups[0]['lr']\n",
    "        })\n",
    "\n",
    "    if len(loader) % accum_steps != 0:\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    avg_loss = running_loss / len(loader.dataset)\n",
    "    train_auroc = roc_auc_score(labels_all, probs_all)\n",
    "    return avg_loss, logits_all, probs_all, labels_all, train_auroc\n",
    "\n",
    "def validate_one_epoch(loader):\n",
    "    model.eval()\n",
    "    logits_all, labels_all, files_all = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y, fn in tqdm(loader, desc=\"  [Validate]\", leave=False):\n",
    "            X = X.to(device)\n",
    "            logits = model(X).squeeze()\n",
    "            logits_all.extend(logits.cpu().numpy())\n",
    "            labels_all.extend(y.numpy())\n",
    "            files_all.extend(fn)\n",
    "\n",
    "    y_true = np.array(labels_all)\n",
    "    y_prob = torch.sigmoid(torch.tensor(logits_all)).cpu().numpy()\n",
    "    val_loss = criterion(\n",
    "        torch.tensor(logits_all),\n",
    "        torch.tensor(labels_all)\n",
    "    ).item()\n",
    "    auroc    = roc_auc_score(y_true, y_prob)\n",
    "\n",
    "    return val_loss, auroc, y_true, y_prob, files_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sME-HYM5GGyC"
   },
   "source": [
    "#Main Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4skBpJI3dLo"
   },
   "source": [
    "**Note about early stop**: In imbalanced medical datasets, loss can decrease even when model performance on the minority class (malignant) degrades. This leads to misleadingly optimistic results. AUROC, on the other hand, evaluates the model's ability to separate the two classes regardless of class imbalance or threshold, making it a more reliable early stopping criterion in this context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8HIWeErPm2SX",
    "outputId": "665b640a-d772-48a0-ab1f-963ab12d0dc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\n",
      "--- Epoch 1/20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  [Train]   :   0%|                                                                            | 0/256 [00:00<?, ?it/s]C:\\Users\\pedro\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ‚Äî Metric & timing trackers ‚Äî\n",
    "epoch_times       = []\n",
    "train_losses      = []\n",
    "train_aurocs      = []\n",
    "val_losses        = []\n",
    "val_aurocs        = []\n",
    "all_epoch_true    = []\n",
    "all_epoch_prob    = []\n",
    "all_epoch_files   = []\n",
    "best_val_auroc    = 0.0\n",
    "best_epoch        = 1\n",
    "\n",
    "print(\"Starting training...\\n\")\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    # ‚Äî Unfreeze at start of epoch 4 ‚Äî\n",
    "    if epoch == freeze_epochs + 1:\n",
    "        print(\">>> Unfreezing stem + block0 now\")\n",
    "        for p in model.conv_stem.parameters():\n",
    "            p.requires_grad = True\n",
    "        for p in model.blocks[0].parameters():\n",
    "            p.requires_grad = True\n",
    "        # no need to rebuild optimizer; param-groups are already set\n",
    "\n",
    "    start = time.time()\n",
    "    print(f\"--- Epoch {epoch}/{n_epochs} ---\")\n",
    "\n",
    "    # === Train (with AMP & grad-acc inside train_one_epoch) ===\n",
    "    train_loss, logits, probs, labels, train_auroc = train_one_epoch(\n",
    "        train_loader,\n",
    "        model,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        scaler,\n",
    "        device,\n",
    "        accum_steps,\n",
    "    )\n",
    "    train_losses.append(train_loss)\n",
    "    train_aurocs.append(train_auroc)\n",
    "\n",
    "    # === Validate ===\n",
    "    val_loss, val_auroc, y_true, y_prob, val_files = validate_one_epoch(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    val_aurocs.append(val_auroc)\n",
    "\n",
    "    # === Store raw predictions for thresholding later ===\n",
    "    all_epoch_true.append(y_true)\n",
    "    all_epoch_prob.append(y_prob)\n",
    "    all_epoch_files.append(val_files)\n",
    "\n",
    "    # === Print summary ===\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train AUROC: {train_auroc:.4f}\")\n",
    "    print(f\"Val   Loss: {val_loss:.4f} | Val   AUROC: {val_auroc:.4f}\\n\")\n",
    "\n",
    "    # === Scheduler step ===\n",
    "    scheduler.step()\n",
    "\n",
    "    # === Timing ===\n",
    "    elapsed = time.time() - start\n",
    "    epoch_times.append(elapsed)\n",
    "    print(f\"Epoch Time: {elapsed:.1f} sec\\n\")\n",
    "\n",
    "    # === Save best model ===\n",
    "    if val_auroc > best_val_auroc:\n",
    "        best_val_auroc = val_auroc\n",
    "        best_epoch     = epoch\n",
    "        torch.save(model.state_dict(), \"checkpoints/best_model.pt\")\n",
    "\n",
    "print(f\"Best AUROC {best_val_auroc:.4f} at epoch {best_epoch}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-a0GHblqGSJo"
   },
   "source": [
    "#Learning Curves & Helper functions and Metric analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l6pBx2CDn3sA"
   },
   "outputs": [],
   "source": [
    "history = pd.DataFrame({\n",
    "    \"epoch\":       list(range(1, n_epochs+1)),\n",
    "    \"train_loss\":  train_losses,\n",
    "    \"train_auroc\": train_aurocs,\n",
    "    \"val_loss\":    val_losses,\n",
    "    \"val_auroc\":   val_aurocs,\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RbHUTaiC1kWb",
    "outputId": "a4c0a4c1-e926-43fe-f5ac-a2f87215cd33"
   },
   "outputs": [],
   "source": [
    "best_auroc_epoch   = history[\"val_auroc\"].idxmax() + 1\n",
    "print(f\"Best AUROC:   {history.val_auroc.max():.4f} at epoch {best_auroc_epoch}\")\n",
    "best_epoch = best_auroc_epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6PA9zoj1oSGt"
   },
   "source": [
    "**Loss vs. Epoch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 490
    },
    "id": "0P7XIx3O_g9m",
    "outputId": "ca4b9831-f351-432b-e441-0afec9960743"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.figure()\n",
    "plt.plot(history.epoch, history.train_loss, label=\"Train Loss\")\n",
    "plt.plot(history.epoch, history.val_loss,   label=\"Val   Loss\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\")\n",
    "plt.legend(); plt.title(\"Loss Curves\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WrQJj-9hq59T"
   },
   "source": [
    "**AUROC vs. Epoch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "JL5zx0IIq5F9",
    "outputId": "9357775c-300e-488c-a601-6616e76100ab"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(history.epoch, history.val_auroc,    label=\"Val   AUROC\")\n",
    "plt.plot(history.epoch, history.train_auroc,    label=\"Train   AUROC\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Metric\")\n",
    "plt.legend(); plt.title(\"Val Metrics Over Epochs\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTePFEQcrSk-"
   },
   "source": [
    "##Threshold Analysis on Final Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BeMCCMo_JmYm"
   },
   "outputs": [],
   "source": [
    "def get_epoch_preds(epoch_idx, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Returns (filenames, y_true, y_prob, y_pred) for epoch `epoch_idx` (0-based),\n",
    "    where y_pred is computed by thresholding y_prob.\n",
    "    \"\"\"\n",
    "    files  = all_epoch_files[epoch_idx]\n",
    "    y_true = np.array(all_epoch_true[epoch_idx])\n",
    "    y_prob = np.array(all_epoch_prob[epoch_idx])\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    return files, y_true, y_prob, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpHsPagYr7Xj"
   },
   "source": [
    "**ROC Curve with Youden Poin**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "hX73-1W6rxZR",
    "outputId": "f07f5bb9-40e6-43f2-fb29-3a918dd074ed"
   },
   "outputs": [],
   "source": [
    "files, y_true, y_prob,_= get_epoch_preds(best_epoch - 1)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_prob)\n",
    "J = tpr - fpr\n",
    "idx = J.argmax()\n",
    "youden_thresh = thresholds[idx]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f\"ROC (AUROC={history.val_auroc.max():.3f})\")\n",
    "plt.scatter(fpr[idx], tpr[idx], color=\"red\", label=f\"Youden={youden_thresh:.2f}\")\n",
    "plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(); plt.title(\"ROC Curve with Youden Point\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27ip4PgKsFz1"
   },
   "source": [
    "**Precision-Recall Curve**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "_xnwnQQdrxbt",
    "outputId": "4de5bd73-e833-408d-b328-827e1c03f5dc"
   },
   "outputs": [],
   "source": [
    "precision, recall, _ = precision_recall_curve(y_true, y_prob)\n",
    "plt.figure()\n",
    "plt.plot(recall, precision)\n",
    "plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O8oPU5dGsMti"
   },
   "source": [
    "**Confusion Matrices at Both Thresholds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "VutXWLjnrxd_",
    "outputId": "174b6651-5176-4a94-bb41-ff5293ff8910"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10,4))\n",
    "for ax, (thr, title) in zip(axes, [(0.5, \"Thresh=0.5\"), (youden_thresh, f\"Youden={youden_thresh:.2f}\")]):\n",
    "    cm = confusion_matrix(y_true, (y_prob >= thr).astype(\n",
    "      int))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", ax=ax,\n",
    "                xticklabels=[\"Benign\",\"Malignant\"], yticklabels=[\"Benign\",\"Malignant\"])\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"True\")\n",
    "plt.suptitle(\"Confusion Matrices\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qCJTBRaSsX77"
   },
   "source": [
    "##Validation Classification Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R1Jn1Bh6sZHn",
    "outputId": "9f5633ce-3fec-45ec-e21f-83e8bd99a59b"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "auroc_at_x = history.val_auroc.iloc[best_epoch - 1]\n",
    "print(f\"AUROC at epoch {best_epoch} = {auroc_at_x:.4f}\")\n",
    "\n",
    "for thr in [0.5, youden_thresh]:\n",
    "    print(f\"--- Classification Report @ threshold = {thr:.2f} ---\")\n",
    "    y_pred_thr = (y_prob >= thr).astype(int)\n",
    "    print(classification_report(y_true, y_pred_thr, target_names=[\"Benign\",\"Malignant\"]))\n",
    "\n",
    "    balacc = balanced_accuracy_score(y_true, y_pred_thr)\n",
    "    macro_f1 = f1_score(y_true, y_pred_thr, average=\"macro\")\n",
    "\n",
    "    print(f\"Balanced Accuracy: {balacc:.3f}\")\n",
    "    print(f\"Macro F1 Score:    {macro_f1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "EVqyuFyc0RTf",
    "outputId": "d73d6922-bd72-4956-ff05-e892f58f7580"
   },
   "outputs": [],
   "source": [
    "plt.plot(epoch_times, marker='o')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Time (s)\")\n",
    "plt.title(\"Epoch Duration\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8u69UUmRVXdq",
    "outputId": "227e3a62-6919-46d0-aed7-4dd69619d7d0"
   },
   "outputs": [],
   "source": [
    "def print_correct_diagnoses(epoch_idx, thresholds=[0.50, 0.37]):\n",
    "    for thresh in thresholds:\n",
    "        print(f\"\\n--- Threshold = {thresh:.2f} ---\")\n",
    "        _, y_true, _, y_pred = get_epoch_preds(epoch_idx, threshold=thresh)\n",
    "\n",
    "        total_malignant = (y_true == 1).sum()\n",
    "        total_benign    = (y_true == 0).sum()\n",
    "\n",
    "        correct_malignant = ((y_true == 1) & (y_pred == 1)).sum()\n",
    "        correct_benign    = ((y_true == 0) & (y_pred == 0)).sum()\n",
    "\n",
    "        pct_malignant = 100 * correct_malignant / total_malignant\n",
    "        pct_benign    = 100 * correct_benign    / total_benign\n",
    "\n",
    "        print(f\"Correct cancer diagnoses:     {correct_malignant}/{total_malignant} = {pct_malignant:.2f}%\")\n",
    "        print(f\"Correct non-cancer diagnoses: {correct_benign}/{total_benign} = {pct_benign:.2f}%\")\n",
    "\n",
    "print_correct_diagnoses(epoch_idx=best_epoch-1, thresholds=[0.50, youden_thresh])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "muNjzs3_BWrT"
   },
   "source": [
    "#Test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UO8YKxbDDf7i"
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-UQ6bmF_f9hp"
   },
   "outputs": [],
   "source": [
    "def compute_auroc_ci(y_true, y_prob, n_bootstraps=1000, seed=42):\n",
    "    \"\"\"\n",
    "    Computes AUROC with a 95% confidence interval using bootstrapping.\n",
    "\n",
    "    Parameters:\n",
    "        y_true (array-like): Ground truth binary labels (0 or 1)\n",
    "        y_prob (array-like): Predicted probabilities\n",
    "        n_bootstraps (int): Number of bootstrap iterations\n",
    "        seed (int): Random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "        auroc (float), ci_lower (float), ci_upper (float)\n",
    "    \"\"\"\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_prob = np.array(y_prob)\n",
    "    auroc = roc_auc_score(y_true, y_prob)\n",
    "\n",
    "    rng = np.random.RandomState(seed)\n",
    "    bootstrapped_scores = []\n",
    "\n",
    "    for _ in range(n_bootstraps):\n",
    "        indices = rng.choice(len(y_true), size=len(y_true), replace=True)\n",
    "        if len(np.unique(y_true[indices])) < 2:\n",
    "            continue\n",
    "        score = roc_auc_score(y_true[indices], y_prob[indices])\n",
    "        bootstrapped_scores.append(score)\n",
    "\n",
    "    ci_lower = np.percentile(bootstrapped_scores, 2.5)\n",
    "    ci_upper = np.percentile(bootstrapped_scores, 97.5)\n",
    "\n",
    "    return auroc, ci_lower, ci_upper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08pllGFdDkkw"
   },
   "source": [
    "##Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mr2_UE7TByC1"
   },
   "outputs": [],
   "source": [
    "THRESHOLD = youden_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qY2L8qqJJver",
    "outputId": "a03e93f7-7cc4-48d4-d2b2-0a83e59f0f73"
   },
   "outputs": [],
   "source": [
    "model = timm.create_model(\n",
    "    \"efficientnet_b0\",\n",
    "    pretrained=True,\n",
    "    num_classes=1\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load(\"checkpoints/best_model.pt\"))\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "COc81yXVDMCX"
   },
   "outputs": [],
   "source": [
    "# 1) Initialize Python lists\n",
    "y_true = []\n",
    "y_prob = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels, _ in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # 2) Forward + sigmoid in one shot, keep as CPU list\n",
    "        outputs = model(inputs).squeeze(-1)        # [B]\n",
    "        probs = torch.sigmoid(outputs).cpu().tolist()\n",
    "\n",
    "        # 3) Extend your lists directly\n",
    "        y_prob.extend(probs)\n",
    "        y_true.extend(labels.cpu().tolist())\n",
    "\n",
    "# 4) Now convert to NumPy (for metrics) if you need:\n",
    "import numpy as np\n",
    "y_true = np.array(y_true)\n",
    "y_prob = np.array(y_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2EWsY47kDut_",
    "outputId": "62a3235d-8146-483c-d20b-a5e41210e6c1"
   },
   "outputs": [],
   "source": [
    "# 1) AUROC + CI\n",
    "test_auroc, ci_lower, ci_upper = compute_auroc_ci(y_true, y_prob)\n",
    "print(f\"AUROC = {test_auroc:.3f} (95% CI: {ci_lower:.3f} ‚Äì {ci_upper:.3f})\")\n",
    "\n",
    "# 2) Find threshold for 95% specificity\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_prob)\n",
    "specificity = 1 - fpr\n",
    "# pick the highest threshold that still gives specificity >= 0.95\n",
    "idx = np.where(specificity >= 0.95)[0]\n",
    "if len(idx) == 0:\n",
    "    tau95 = 0.5  # fallback\n",
    "else:\n",
    "    tau95 = thresholds[idx[-1]]\n",
    "print(f\"Threshold for 95% specificity: {tau95:.3f}\")\n",
    "\n",
    "# 3) Binarize at that threshold\n",
    "test_y_pred95 = (y_prob >= tau95).astype(int)\n",
    "\n",
    "# 4) Other metrics at œÑ95\n",
    "test_recall95 = recall_score(y_true, test_y_pred95)\n",
    "test_bal_acc  = balanced_accuracy_score(y_true, test_y_pred95)\n",
    "test_macro_f1 = f1_score(y_true, test_y_pred95, average=\"macro\")\n",
    "\n",
    "print(f\"Recall @ 95% Spec:           {test_recall95:.3f}\")\n",
    "print(f\"Balanced Accuracy @ œÑ95:    {test_bal_acc:.3f}\")\n",
    "print(f\"Macro F1 Score @ œÑ95:       {test_macro_f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k0wPcw19vgeA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "VVFYbmS5PjXk",
    "shSr8dCOmPHz",
    "ORwNNfl86qiq",
    "p3pAAg87FQDg"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2003a0be287c4d16adb940d3b7a0ae3c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "38d1f87310274329a63985e9ff882c4d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2003a0be287c4d16adb940d3b7a0ae3c",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_8fd9c0423e814e6192edc836606a0746",
      "value": "‚Äá21.4M/21.4M‚Äá[00:00&lt;00:00,‚Äá94.9MB/s]"
     }
    },
    "3d1d8de42f0849bf9788e289478e8bfa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_89830d71704449439a71a2e309dbca09",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_e3f5c3ab67b94e409c72f51d704c8869",
      "value": "model.safetensors:‚Äá100%"
     }
    },
    "44aef92d50654c11ac0e2b1381356f72": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3d1d8de42f0849bf9788e289478e8bfa",
       "IPY_MODEL_f638ba6378504fd9a9e6a2cb79086e23",
       "IPY_MODEL_38d1f87310274329a63985e9ff882c4d"
      ],
      "layout": "IPY_MODEL_576848c740094aac9b310c497da9bb03"
     }
    },
    "576848c740094aac9b310c497da9bb03": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6c7fad346ca740bfa807fc5be8399ce2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "89830d71704449439a71a2e309dbca09": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8fd9c0423e814e6192edc836606a0746": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e3f5c3ab67b94e409c72f51d704c8869": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e8cfee561f1d4ea9b3207603b7443535": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f638ba6378504fd9a9e6a2cb79086e23": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6c7fad346ca740bfa807fc5be8399ce2",
      "max": 21355344,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e8cfee561f1d4ea9b3207603b7443535",
      "value": 21355344
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
